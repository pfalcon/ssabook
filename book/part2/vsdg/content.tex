\chapter{Graphs and gating functions \Author{J. Stanier}}
\numberofpages{15}
\graphicspath{{img/}{vsdg/img/}{part2/vsdg/img/}}

\newcommand{\Gn}{$\gamma$-node}
\newcommand{\Gns}{$\gamma$-nodes}
\newcommand{\Tn}{$\theta$-node}
\newcommand{\Tns}{$\theta$-nodes}
\newcommand{\Ttns}{$\theta^{\mathit{tail}}$-nodes}
\newcommand{\triVM}{\textit{tri}VM}

\newcommand{\instruction}[1]{\texttt{#1}}
\newcommand{\register}[1]{\texttt{v#1}}

\section{Introduction}

Many compilers represent the input program as some form of graph in order to aid analysis and transformation. A cornucopia of program graphs have been presented in the literature and implemented in real compilers. Therefore it comes as no surprise that a number of program graphs use SSA concepts as the core principle of their representation. These range from very literal translations of SSA into graph form, to more abstract graphs which are implicitly SSA. We aim to introduce a selection of program graphs which use SSA concepts, and examine how they may be useful to a compiler writer.

One of the seminal graph representations is the Control Flow Graph (CFG), which was introduced by Allen to explicitly represent possible control paths in a program. Traditionally, the CFG is used to convert a program into SSA form. Additionally, representing a program in this way makes a number of operations simpler to perform, such as identifying loops, discovering irreducibility and performing interval analysis techniques.

The CFG models control flow, but many graphs model \textit{data flow}. This is useful as a large number of compiler optimizations are based on data flow. The graphs we consider in this section are all data flow graphs, representing the data dependences in a program. We will look at a number of different SSA-based graph representations. These range from those which are a very literal translation of SSA into a graph form to those which are more abstract in nature. An introduction to each graph will be given, along with diagrams to show how sample programs look when translated into that particular graph. Additionally, we will touch on the literature describing the usage of a given graph with the application that it was used for.

\section{The SSA Graph}

We begin our exploration with a graph that is very similar to SSA: the SSA Graph. Many different variations exist in the literature, so we take ours from Cooper et al. An SSA Graph consists of vertices which represent operations (such as \texttt{add} and \texttt{load}) or $\mathtt{\phi}$-functions, and directed edges connect uses to definitions. The edges to a vertex represent the arguments required for that operation, and the edge from a vertex represents the propagation of that operation's result after it has been computed. This graph is therefore a \textit{demand-based} representation. In order to compute a vertex, we must first \textit{demand} the results of the operands and then perform the operation indicated on that vertex. The SSA Graph can be constructed from a program in SSA form by adding use-definition chains. We present some sample code in Figure~\ref{fig: ssa-graph-example-code} which is then translated into an SSA Graph. Note that there are no explicit nodes for variables in the graph. Instead, an operator node can be seen as the ``location'' of the value stored in a variable. We have annotated operators with variable names to show the correspondence between the operations in the graph and in the SSA form program.

\begin{figure}[ht]
\centering
\subfigure{
  \begin{minipage}[b]{0.3\linewidth}
    \texttt{\begin{tabbing}
	begin: \=$a_{0}$ = 0; \\
	\> $i_{0}$ = 0; \\
	loop: \> $a_{1}$ = $\phi$($a_{0}$,$a_{2}$); \\
	\> $i_{1}$ = $\phi$($i_{0}$,$i_{2}$); \\
	\> $a_{2}$ = $a_{1}$ * $i_{1}$; \\
	\> $i_{2}$ = $i_{1}$ + 1; \\
	\> if $i_{2}$ < 100 goto loop;\\
	end: \> $a_{3}$ = $\phi$($a_{1}$,$a_{2}$); \\
	\> $i_{3}$ = $\phi$($i_{1}$,$i_{2}$); \\
	\> print($a_{3}$ + $i_{3}$);
      \end{tabbing}}
  \end{minipage}
}
\subfigure{
  \begin{minipage}[b]{0.3\linewidth}
    \includegraphics[scale=0.5]{ssa-graph.pdf}
  \end{minipage}
}
\caption{Some SSA code translated into an SSA graph.}
\label{fig: ssa-graph-example-code}
\end{figure}

The primary benefit of representing the input program in this form is that the compiler writer is able to apply a wide array of graph-based optimizations by using standard graph traversal and transformation techniques. It is possible to augment the SSA Graph to model memory dependencies. This is achieved by adding additional \textit{state edges} that enforce an order of interpretation. These edges are extensively used in the Value State Dependence Graph, which we will look at later, after we touch on the concept of gating functions.

In the literature, the SSA Graph has been used to detect a variety of induction variables in loops, also for performing instruction selection techniques, operator strength reduction, rematerialization, and has been combined with an extended SSA language to aid compilation in a parallelizing compiler. The reader should note that the exact specification of what constitutes an SSA Graph changes from paper to paper. The essence of the IR has been presented here, as each author tends to make small modifications for their particular implementation.

\section{Program Dependence Graph}

The Program Dependence Graph (PDG) represents both control and data dependencies together in one graph. The PDG was developed to aid optimisations requiring reordering of instructions and graph rewriting for parallelism, as the strict ordering of the CFG is relaxed and complemented by the presence of data dependence information. The PDG is a directed graph $G=(V,E)$ where nodes $V$ are statements, predicate expressions or region nodes, and edges $E$ represent either control or data dependencies. Thus, the set of all edges $E$ has two distinct subsets: the control dependence subgraph $E_{C}$ and the data dependence subgraph $E_{D}$. $E_{C}$ can be cyclic if a loop is present in the program, since a loop in the PDG is defined by a control back edge forming a strongly connected region. $E_{D}$ is always acyclic, and can be seen as a series of data dependency DAGs for each basic block, which are then connected together based on the data flow through the program. Similar to the CFG, a PDG also has two nodes $\mathtt{ENTRY}$ and $\mathtt{EXIT}$, through which data flow enters and exits the program respectively.

Statement nodes represent instructions in the program. Predicate nodes test a conditional statement and have {\sf true} and {\sf false} edges to represent the choice taken on evaluation of the predicate. Region nodes group all nodes with the same control dependencies together, and order them into a hierarchy. If the control dependence for a region node is satisfied, then it follows that all of its children can be executed. Thus, if a region node has three different control-independent statements as immediate children, then these could potentially be executed in parallel. We show example code translated into a PDG in Figure~\ref{fig:pdg}. Rectangular nodes represent statements, diamond nodes predicates, and circular nodes are region nodes. Filled edges represent data dependence, and dashed edges represent control dependence.

\begin{figure}
\centering
\subfigure{
  \begin{minipage}[b]{0.3\linewidth}
    \texttt{\begin{tabbing}
        a = b + c;\\
        x = a * d;\\
        if(x \= == y)\\
        \> z = e;\\
        else\\
        \> z = f;\\
        y = z + 1;\\
        return y;\\
      \end{tabbing}}
  \end{minipage}
}
\subfigure{
  \includegraphics[scale=0.4]{img/pdg.pdf}
}
\caption{Some code translated into a PDG.}
\label{fig:pdg}
\end{figure}OA

% TODO: expand this construction section
Construction of the PDG is tackled in two steps from the CFG: construction of the control dependence subgraph and construction of the data dependence subgraph. Ferrante et al. construct the control dependence subgraph in $O(N^{2})$ time. The data dependence subgraph can be constructed after aliasing, procedure calls and side effects are analysed in the program. This involves constructing a DAG for each basic block and then linking them together. Harrold et al. construct the PDG during parsing.

Ferrante et al. considered the problem of generating linear code from a PDG, showing that generating the {\it minimal size} CFG from a PDG is an NP-Complete problem. Further work has improved on the efficiency and generality of generating sequential code. The PDG's structure has been exploited for generating code for vectorisation, and has also been used in order to perform accurate program slicing and testing.

\section{Gating functions}

In SSA form, $\phi$-functions are used to identify points where variable definitions converge. However, they cannot be directly \textit{interpreted}, as they do not specify the condition which determines which of the variable definitions to choose. By this logic, we cannot directly interpret the SSA Graph. Being able to interpret our IR is a useful property as it gives the compiler writer more information when implementing optimizations, and also reduces the complexity of performing code generation. Gated Single Assignment form is an extension of SSA with \textit{gating functions}. These gating functions are directly interpretable versions of $\phi$-nodes, and replace $\phi$-nodes in the representation. There are three forms of gating function and we take our definition from Tu and Padua:

\begin{itemize}
\item The $\gamma$ function explicitly represents the condition which determines which $\phi$ value to select. A $\gamma$ function is of the form $\gamma(P,V_{1},V_{2})$ where $P$ is a predicate, and $V_{1}$ and $V_{2}$ are the values to be selected if the predicate evaluates to true or false respectively. This can be read simply as \textit{if-then-else}. 
\item The $\mu$ function is inserted at loop headers to select the initial and loop carried values. A $\mu$ function is of the form $\mu(V_{init},V_{iter})$, where $V_{init}$ is the initial input value for the loop, and $V_{iter}$ is the iterative input. We replace $\phi$-functions at loop headers with $\mu$ functions.
\item The $\eta$ function determines the value of a variable when a loop terminates. An $\eta$ function is of the form $\eta(P,V_{final})$ where $P$ is a predicate and $V_{final}$ is the definition reaching beyond the loop.
\end{itemize}

It is easiest to understand these gating functions by means of an example. Figure~\ref{fig: gsa-graph-example} shows how our earlier code in Figure~\ref{fig: ssa-graph-example-code} translates into GSA form. Here, we can see the use of both $\mu$ and $\eta$ gating functions. At the header of our sample loop the $\phi$-functions have been replaced by $\mu$ functions which determines between the initial and iterative values of \texttt{a} and \texttt{i}. After the loop has finished executing, the two $\eta$ functions propagate the correct value from the corresponding $\mu$ function. % TODO: more explanation of the diagram here

These gating functions are important as the concept will form components of the Value State Dependence Graph later. GSA has seen a number of uses in the literature. The original usage of GSA was by Ballance et al. as an intermediate stage in the construction of the Program Dependence Web IR. Havlak \cite{Havlak93constructionof} presented an algorithm for construction of a simpler version of GSA -- Thinned GSA -- which is constructed from a CFG in SSA form. Tu and Padua presented an algorithm that constructs SSA and GSA simultaneously in a single process.

\begin{figure}
\centering
\includegraphics[scale=0.55]{gsa-example.pdf}
\caption{A graph representation of our sample code in GSA form.}
\label{fig: gsa-graph-example}
\end{figure}

By using gating functions it becomes possible to construct IRs based solely on data dependencies. These IRs are sparse in nature compared to the CFG, making them good for analysis and transformation. This is also a more attractive proposition than generating and maintaining both a control flow graph and data flow graph, which can be complex and prone to human error. One approach has been to combine both of these into one representation, as is done in the Program Dependence Graph \cite{24041}. Alternatively, we can utilize gating functions along with a data flow graph for an effective way of representing whole program information using data flow information.

\section{Value State Dependence Graph}

The gating functions defined in the previous section were used in the development of a sparse data flow graph IR called the Value Dependence Graph (VDG), which was then further refined into the Value State Dependence Graph (VSDG). The Value State Dependence Graph is a directed graph consisting of operation
nodes, loop and merge nodes together with value- and state-dependency edges.
Cycles are permitted but 
must satisfy various restrictions.
A VSDG represents a single procedure; this matches the classical CFG but
differs from the VDG in which loops were converted to tail-recursive procedures
called at the logical start of the loop. 

An example VSDG is shown in Figure~\ref{fig:fac}.  In (a) we have the original
C source for a recursive factorial function.  The corresponding VSDG
(b) shows both value and state edges and a selection of nodes.

\begin{figure}[!htb]
\centering
\begin{tabular}{ccc}	%\cline{1-1}
\begin{minipage}[c][\height][t]{2.2in}
\texttt{\begin{tabbing}
int \=fac(int n) \{\\
\> int result;\\
\> if(\=n == 1)\\
\> \> result = n;\\
\> else\\
\> \> result = n * fac(n - 1);\\
\> return result;\\
\}
\end{tabbing}}
\end{minipage}
& \hspace{0.25in} &
\begin{minipage}[c][\height][b]{2.2in}
\includegraphics[scale=0.9]{example-fac}
\end{minipage}	\\  %\cline{1-1}
& & \\
(a) & & (b)	\\ 
\end{tabular}

\caption{A recursive factorial function, whose VSDG illustrates the key graph
components---value dependency edges (solid lines), state dependency edges
(dashed lines), a \instruction{const} node, a \instruction{call} node, two
\Gns, a conditional node,and the function entry and exit
nodes.}
\label{fig:fac}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Definition of the VSDG}

A VSDG is a labelled directed graph $G=(N,E_V,E_S,\ell,N_0,N_\infty)$
consisting of nodes $N$
(with unique entry node $N_0$ and exit node $N_\infty$),
value-dependency edges $E_V \subseteq N \times N$,
state-dependency edges $E_S \subseteq N \times N$.
The labelling function $\ell$ associates each node with an operator.

The VSDG corresponds to a structured program, e.g.
that there are no cycles in the VSDG except those mediated by $\theta$ (loop)
nodes.

Value dependency ($E_V$) indicates the flow of values between nodes. State dependency ($E_S$) represents two things; the first is
essential sequential dependency required by the original program, e.g.
a given load instruction may be required to follow a given store instruction
without being re-ordered, and a {\tt return} node in general must wait for an
earlier loop to terminate even though there might be no value-dependency
between the loop and the {\tt return} node.
The second purpose is that
state-dependency edges can be added incrementally until the VSDG corresponds to
a unique CFG.
Such state dependency edges are called {\em serializing} edges.

The VSDG inherits from the VDG the property that a program is implicitly
represented in Static Single Assignment (SSA) form\cite{115320}: a
given operator node, $n$, will have zero or more $E_V$-successors using its
value.  Note that, in implementation terms, a single register can hold the
produced value for consumption at all successors; it is therefore useful to
talk about the idea of an output {\em port} for $n$ being allocated a specific
register, $r$, to abbreviate the idea of $r$ being used for each edge $(n_1,n_2)$
where $n_2 \in \mathit{succ}(n_1)$.  Similarly, we will talk about (say) the
``right-hand input port'' of a subtraction instruction, or of the $R$-input of
a \Tn.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%		NODE PROPERTIES
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Node Labelling with Instructions}

There are four main classes of VSDG nodes: value nodes (representing pure
arithmetic),
\Gns\ (conditionals),  \Tns\ (loops), and state nodes (side-effects).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Value Nodes}

The majority of nodes in a VSDG generate a value based on some computation
(add, subtract, etc) applied to their dependent values (constant nodes,
which have no dependent nodes, are a special case).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{$\gamma$-Nodes}

The \Gn\ is similar to the $\gamma$ gating function in being dependent on
a control predicate, rather than the control-independent nature of SSA
$\phi$-functions.
%

A \Gn\ $\gamma(C, T, F)$ evaluates the condition dependency $C$, and
returns the value of $T$ if $C$ is true, otherwise $F$.

%
We generally treat \Gns\ as single-valued nodes (constrast \Tns, which are
treated as tuples), with the effect that two separate \Gns\ with the same
condition can be later combined into a tuple
using a single test. 
Figure~\ref{fig:twinPhis} illustrates two \Gns\ that can be combined in
this way.

\begin{figure}[!hb]
\centering
\begin{tabular}{ccc}	\cline{1-1}
\begin{minipage}[l]{2.0in}
\begin{verbatim}

a)   if (P)
        x = 2, y = 3;
    else
        x = 4, y = 5;

b)   if (P) x = 2; else x = 4;
       ...
    if (P) y = 3; else y = 5;

\end{verbatim}
\end{minipage}	
& \hspace {0.2in}
\begin{minipage}[m][\height][l]{1.6in}
\includegraphics[scale=0.9]{vsdg-gammanodes}
\end{minipage} \\ \cline{1-1}
\end{tabular}

\caption{Two different code schemes (a) \& (b) map to the same
\Gn\ structure.}

\label{fig:twinPhis}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{$\theta$-Nodes}

The \Tn\ models the iterative behaviour of loops, modelling loop state with the
notion of an \emph{internal value} which may be updated on each iteration of
the loop.  It has five specific ports which represent dependencies at various
stages of computation.
%

A \Tn\ $\theta(C,I,R,L,X)$ sets its internal value to initial value
$I$ then, while condition value $C$ holds true, sets $L$ to the current
internal value and updates the internal value with the repeat value $R$.  When
$C$ evaluates to false computation ceases and the last internal value is
returned through the $X$ port.

%
A loop which updates $k$ variables will have:
a single condition port $C$,
initial-value ports $I_1,\ldots,I_k$,
loop iteration ports $L_1,\ldots,L_k$,
loop return ports $R_1,\ldots,R_k$, and
loop exit ports $X_1,\ldots,X_k$.
The example in Figure~\ref{fig:thetatuple} shows a pair (2-tuple) of values being
used for $I,R,L,X$, one for each loop-variant value.

For some purposes the $L$ and $X$ ports could be fused,
as both represent outputs
within, or exiting, a loop (the values are identical, while the $C$ input
merely selects their routing).  We avoid this for two reasons:
({\it i\/}) we have operational semantics for VSDGs $G$
and these semantics require separation of these concerns; and
({\it ii\/}) our construction of $G^\mathit{noloop}$ requires it.

\begin{figure}[!ht]
\centering
\begin{tabular}{cc} \cline{1-1}
\begin{minipage}[c][\height][t]{0.9in}
\begin{verbatim}
j = ...
for( i = 0; 
    i < 10; 
    ++i )
   --j;
	
... = j;
\end{verbatim}
\end{minipage}
&
\begin{minipage}[c][\height][b]{3.0in}
\includegraphics[scale=0.6]{vsdg-theta}
\end{minipage} \\ \cline{1-1}
\end{tabular}

\caption{An example showing a \texttt{for} loop. Evaluating the \textbf{X} port triggers it to evaluate the \textbf{I} value (outputting the value on the \textbf{L} port). While \textbf{C} evaluates to true, it evaluates the \textbf{R} value (which in this case also uses the $\theta$-node's \textbf{L} value). When \textbf{C} is false, it returns the final internal value through the \textbf{X} port. As \texttt{i} is not used after the loop there is is no dependency on the \texttt{i} port of \textbf{X}.}
\label{fig:thetatuple}
\end{figure}

The \Tn\ directly implements pre-test loops (\instruction{while},
\instruction{for}); post-test loops (\instruction{do...while},
\instruction{repeat...until}) are synthesised from a pre-test loop preceded by
a duplicate of the loop body.
At first this may seem to cause unnecessary duplication of code, but it has two
important benefits:
({\it i\/}) it exposes the first loop body iteration to optimization in post-test
loops (cf. loop-peeling), and
({\it ii\/}) it normalizes all loops to one loop structure, which both reduces the
cost of optimization, and increases the likelihood of two schematically-dissimilar
loops being isomorphic in the VSDG.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{State Nodes}

Loads, stores, and their volatile equivalents, compute a value and/or state
(non-volatile loads return a value from memory without generating a new
state).  Accesses to volatile memory or hardware can change state independently
of compiler-aware reads or writes. 

The \instruction{call} node takes both the name of the function to call and a
list of arguments, and returns a list of results; it is treated as a state
node as the function body may read or update state.

\medskip

We maintain the simplicity of the VSDG by imposing the restriction that
\emph{all} functions have \emph{one} return node (the exit node $N_\infty$),
which returns at least one result (which will be a state value in the case of
\instruction{void} functions).
To ensure that function calls and definitions are able to be allocated registers easily, we suppose
that the number of arguments to, and results from, a function is smaller
than the number of physical registers---further arguments can be passed
via a stack as usual.

Note also that the VSDG neither forces loop invariant code into nor out-of loop
bodies, but rather allows later phases to determine, by adding serializing edges, such
placement  of loop invariant nodes for later
phases.

\section{Summary}

A compiler's intermediate representation can be a graph, and many different graphs exist in the literature. We can represent the control flow of a program as a Control Flow Graph (CFG) \cite{808479}, where straight-line instructions are contained within basic blocks and edges show where the flow of control may be transferred to once leaving that block. A CFG is traditionally used to convert a program to SSA form \cite{115320}. We can also represent programs as a type of data flow graphs, and SSA can be represented in this way as an SSA Graph \cite{504710}. The SSA Graph has been used to detect a variety of induction variables in loops \cite{143131,201003}, also for performing instruction selection techniques \cite{1375663,1269857}, operator strength reduction \cite{504710}, rematerialization \cite{143143}, and has been combined with an extended SSA language to aid compilation in a parallelizing compiler \cite{Stoltz_extendedssa}.

Gating functions can be used to create directly interpretable $\phi$-functions. These are used in Gated Single Assignment Form \cite{93578,207115}.

We then described the Value State Dependence Graph (VSDG) \cite{UCAM-CL-TR-607}, which is an improvement on the Value Dependence Graph \cite{177907}. It uses the concept of gating functions, data dependencies and state to model a program. Detailed semantics of the VSDG are available \cite{UCAM-CL-TR-607}, as well as semantics of a related IR: the Gated Data Dependence Graph \cite{upton}. Further study has taken place on the problem of generating code from the VSDG \cite{DBLP:conf/pdpta/Upton03,UCAM-CL-TR-705}, and it has also been used to perform a combined register allocation and code motion algorithm \cite{Johnson_combinedcode}.